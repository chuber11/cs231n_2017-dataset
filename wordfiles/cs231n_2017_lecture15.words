hrsof
DSDAbs
LR
DesignWare
CONV
ValueCountRecover
Paulius
MeanEnergy
KnowledgeKnowledge
Arithm
P-fold
sparsities
FPGADaDianNao
Datacenter
activationsbiOutput
Ptr
RegularizationIandola
thesearXiv
FPGAASIC
re-trained
Xeon
DNN
Bxyjx
DMA
Verilog
Tensorizing
orientedprogrammable
RegularizationGeo
Micikevicius
NeuralTalk
NetworksModels
Vinyals
magnitude-based
GPU
DSD
MKL
MeanSpeedupCPU
Hotchips
CBLAS
Lavin
NVLink
MULT
MemCompressed
rfor
TCO
functionWeightsTPU
FFN
TWN
uint
MiB
mixed-precision
SpmatRead
DNNs
arXiv
ConstraintSparse
HardwareAgenda
SqueezeNet
ResNet
bwd
domainCPU
AxykKernels
lget
FullyConnected
logicHardware
Salishan
rwe
byfine-tuned
Visual-Semantic
NetworkOriginal
teraEops
pointFixed
EfficiencyCPU
nlog
TransformationPart
bbits
In-frequent
data-center
AijAijBxyjx
BxyjBxyjBxyjBxyjBxyj
TSMC
EIE
accuracyCloud
Forall
VT
un-truncated
Deepface
cuDNN
weightsQuantization
huizi
perdavan
tto
energyWhere
TransformFilterData
BankReLUAct
hyper-parameters
ISCA
Zhu
ComputingMobile
Fully-connected
WeightNormalized
TPU
XPERIMENTAL
RegularizationInput
modelChallenge
Nikolaev
Co-Design
Synopsys
Wihas
songhan
centroids
Parallelization
nvidia-smi
ters
FMAs
commercially-available
ASICASIC
mGPU
Tegra
tfor
LeCun
beseline
RegsAct
SqueezeNetDeep
fb
IndexHuffman
off-chip
SRAM
Vanhoucke
GoogLeNet
HashedNets
Feb
uttenlocher
UAT
AlexNet-level
Karpathy
RegularizationDataflow
NT-Wd
CCCCCCCCCCCCCAReLU
Iandola
GTX
VGG
withW
LNZD
ExaScale-ClassComputer
HardwareAlgorithm
Point-wise
cuBLAS
NVIDIASee
Top-k
RegularizationCan
multiplicationOutput
Wij
AverageResults
WeightFinal
Krizhevsky
eDRAMSparse
Actv
meta-data
TrainingUnder
combinational
CUDA
Demouth
GEMM
WeightsCluster
HardwareWrap-Up
Algorithm-Hardware
landWn
SRAMAct
petaEops
TDP
UANTIZATION
networkEncode
Nvcaffe
Speedups
pcm-power
ASICTrueNorth
HBM
CPUGPU
FCConvVectorPoolTotal
Hyper-Parameter
GFLOP
StanfordChief
cuSPARSE
sparse-matrix
BxyjBxyj
AccuracyCompressed
ZitnickImage
TFLOPS
ValueCount
multiply-accumulate
LSTMs
Szegedy
re-dense
kclusters
vations
ImageResult
GPGPUFPGA
ETWORK
DGX
ISSCC
Str
ConvNets
small-weight
ASICEIE
AccumPrediction
iis
AijAij
AccumDest
tivation
AddrAct
WSJ
GEMV
CCCCCCCCCCCCCA
SizeCompressed
AddressBypassLeading
SGD
TrainingAlgorithm
matrix-vector
PtrRead
IC
accuracyoriginal
pjand
ValueCountPruning
NT-We
JohnsonFei-Fei
CSRMV
full-resolution
SVD
FP
sparsity-constrained
ActRW
multicore
RegularizationHardware
Actqueue
Eyeriss
ingBrain-Inspired
FWD
Qiu
gate-level
Ijfrom
modelHow
CCU
CPUTitanX
AI-First
NTRODUCTION
DaDiannao
multi-speaker
LSTM
Wwith
NT-LSTM
sparsity
SpMat
lation
BookPruning
Hassibi
TrainingAgenda
kshared
AijModel-Parallel
tationally
AccessSparse
Sze
AlexNet
k-th
Re-DenseDense
Nparameters
AlexNetDeep
cycle-accurate
Model-Parallel
andCompression
RatioOriginal
SizeCompression
VGGNet
SPBLAS
reinitialized
TPUs
Kuvkj
Haswell-E
CTCCost
TTQ
Rooflines
y-v
CC-BY
GPUTegra
ASICASICASIC
TimeTrained
bimodal
ArchitectureEIE
NZero
ArithmUnit
sparisty
over-parameterized
Bxyj
PEINEIE
xyl
FMA
RNN
x-u
HPC
CodingZhu
ASIC
previously-pruned
lenables
In-Data
RobotsMachine
H-tree
UnitRegsCol
Fei-Fei
IndexSrc
Andrej
ICLR
RTL
pselects
jingpu
CPUs
Axyk
ValueCountTrain
andEIE
Auto-Caption
CodingPublished
BxyjBxyjBxyj
xis
Skylake
Comput
nconnections
ferent
PruningPruning
IEEE
RNNs
matrixInput
image-net
mGPUA-Eye
orientedthroughput
ByteTPU
describedTable
NV
